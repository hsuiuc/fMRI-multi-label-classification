{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import scipy.io\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from six.moves import range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"train_X.npy\")\n",
    "Y_train = np.load(\"train_binary_Y.npy\")\n",
    "X_test = np.load(\"valid_test_X.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "nb_classes = 19\n",
    "nb_epoch = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(26, 31, 2..., activation=\"relu\")`\n",
      "C:\\Anaconda\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    " \n",
    "X_train = np.load(\"train_X.npy\")\n",
    "Y_train = np.load(\"train_binary_Y.npy\")\n",
    "X_test = np.load(\"valid_test_X.npy\")\n",
    " \n",
    "# 5. Preprocess input data\n",
    "#X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    " \n",
    "# 6. Preprocess class labels\n",
    "#Y_train = np_utils.to_categorical(y_train, 10)\n",
    "#Y_test = np_utils.to_categorical(y_test, 10)\n",
    " \n",
    "# 7. Define model architecture\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(26,31,23)))\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.add(Dense(nb_classes,activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\py35\\lib\\site-packages\\keras\\models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4602/4602 [==============================] - 13s - loss: 6.5517    \n",
      "Epoch 2/150\n",
      "4602/4602 [==============================] - 13s - loss: 5.7444    \n",
      "Epoch 3/150\n",
      "4602/4602 [==============================] - 13s - loss: 5.3103    \n",
      "Epoch 4/150\n",
      "4602/4602 [==============================] - 12s - loss: 4.9787    \n",
      "Epoch 5/150\n",
      "4602/4602 [==============================] - 13s - loss: 4.7276    \n",
      "Epoch 6/150\n",
      "4602/4602 [==============================] - 12s - loss: 4.5370    \n",
      "Epoch 7/150\n",
      "4602/4602 [==============================] - 12s - loss: 4.3971    \n",
      "Epoch 8/150\n",
      "4602/4602 [==============================] - 12s - loss: 4.2945    \n",
      "Epoch 9/150\n",
      "4602/4602 [==============================] - 12s - loss: 4.1967    \n",
      "Epoch 10/150\n",
      "4602/4602 [==============================] - 13s - loss: 4.0467    \n",
      "Epoch 11/150\n",
      "4602/4602 [==============================] - 13s - loss: 3.9258    \n",
      "Epoch 12/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.8408    \n",
      "Epoch 13/150\n",
      "4602/4602 [==============================] - 13s - loss: 3.7552    \n",
      "Epoch 14/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.6808    \n",
      "Epoch 15/150\n",
      "4602/4602 [==============================] - 13s - loss: 3.6193    \n",
      "Epoch 16/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.5968    \n",
      "Epoch 17/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.5302    \n",
      "Epoch 18/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.4910    \n",
      "Epoch 19/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.4679    \n",
      "Epoch 20/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.4119    \n",
      "Epoch 21/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.3825    \n",
      "Epoch 22/150\n",
      "4602/4602 [==============================] - 13s - loss: 3.3654    \n",
      "Epoch 23/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.3495    \n",
      "Epoch 24/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.3165    \n",
      "Epoch 25/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.2837    \n",
      "Epoch 26/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.2446    \n",
      "Epoch 27/150\n",
      "4602/4602 [==============================] - 13s - loss: 3.2475    \n",
      "Epoch 28/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.2169    \n",
      "Epoch 29/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.1925    \n",
      "Epoch 30/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.1721    \n",
      "Epoch 31/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.1665    \n",
      "Epoch 32/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.1545    \n",
      "Epoch 33/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.1223    \n",
      "Epoch 34/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.1044    \n",
      "Epoch 35/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.0668    \n",
      "Epoch 36/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.0518    \n",
      "Epoch 37/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.0520    \n",
      "Epoch 38/150\n",
      "4602/4602 [==============================] - 12s - loss: 3.0413    \n",
      "Epoch 39/150\n",
      "4602/4602 [==============================] - 13s - loss: 3.0173    \n",
      "Epoch 40/150\n",
      "4602/4602 [==============================] - 13s - loss: 3.0004    \n",
      "Epoch 41/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.9842    \n",
      "Epoch 42/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.9855    \n",
      "Epoch 43/150\n",
      "4602/4602 [==============================] - 14s - loss: 2.9704    \n",
      "Epoch 44/150\n",
      "4602/4602 [==============================] - 14s - loss: 2.9714    \n",
      "Epoch 45/150\n",
      "4602/4602 [==============================] - 14s - loss: 2.9612    \n",
      "Epoch 46/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.9201    \n",
      "Epoch 47/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.9060    \n",
      "Epoch 48/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.9138    \n",
      "Epoch 49/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.8965    \n",
      "Epoch 50/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.8839    \n",
      "Epoch 51/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8765    \n",
      "Epoch 52/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8700    \n",
      "Epoch 53/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8641    \n",
      "Epoch 54/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8372    \n",
      "Epoch 55/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8324    \n",
      "Epoch 56/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.8190    \n",
      "Epoch 57/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8299    \n",
      "Epoch 58/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8435    \n",
      "Epoch 59/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8082    \n",
      "Epoch 60/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.8063    \n",
      "Epoch 61/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7927    \n",
      "Epoch 62/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7849    \n",
      "Epoch 63/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7985    \n",
      "Epoch 64/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7921    \n",
      "Epoch 65/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7770    \n",
      "Epoch 66/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7696    \n",
      "Epoch 67/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.7723    \n",
      "Epoch 68/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7734    \n",
      "Epoch 69/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7522    \n",
      "Epoch 70/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7433    \n",
      "Epoch 71/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7452    \n",
      "Epoch 72/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7249    \n",
      "Epoch 73/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7326    \n",
      "Epoch 74/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7283    \n",
      "Epoch 75/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7345    \n",
      "Epoch 76/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7306    \n",
      "Epoch 77/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7126    \n",
      "Epoch 78/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.7044    \n",
      "Epoch 79/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.7014    \n",
      "Epoch 80/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.6997    \n",
      "Epoch 81/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.6981    \n",
      "Epoch 82/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.6955    \n",
      "Epoch 83/150\n",
      "4602/4602 [==============================] - 13s - loss: 2.6866    \n",
      "Epoch 84/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6926    \n",
      "Epoch 85/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6859    \n",
      "Epoch 86/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6786    \n",
      "Epoch 87/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6769    \n",
      "Epoch 88/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6722    \n",
      "Epoch 89/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6670    \n",
      "Epoch 90/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6799    \n",
      "Epoch 91/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6601    \n",
      "Epoch 92/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6604    \n",
      "Epoch 93/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6554    \n",
      "Epoch 94/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6617    \n",
      "Epoch 95/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6496    \n",
      "Epoch 96/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6520    \n",
      "Epoch 97/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6317    \n",
      "Epoch 98/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6360    \n",
      "Epoch 99/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6409    \n",
      "Epoch 100/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6443    \n",
      "Epoch 101/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6262    \n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4602/4602 [==============================] - 12s - loss: 2.6357    \n",
      "Epoch 103/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6271    \n",
      "Epoch 104/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6303    \n",
      "Epoch 105/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6464    \n",
      "Epoch 106/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6310    \n",
      "Epoch 107/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6254    \n",
      "Epoch 108/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6185    \n",
      "Epoch 109/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6279    \n",
      "Epoch 110/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6280    \n",
      "Epoch 111/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6220    \n",
      "Epoch 112/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6132    \n",
      "Epoch 113/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6020    \n",
      "Epoch 114/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6169    \n",
      "Epoch 115/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.6138    \n",
      "Epoch 116/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6124    \n",
      "Epoch 117/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6042    \n",
      "Epoch 118/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6157    \n",
      "Epoch 119/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6010    \n",
      "Epoch 120/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.6030    \n",
      "Epoch 121/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5935    \n",
      "Epoch 122/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5932    \n",
      "Epoch 123/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5831    \n",
      "Epoch 124/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5952    \n",
      "Epoch 125/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.5916    \n",
      "Epoch 126/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5943    \n",
      "Epoch 127/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5885    \n",
      "Epoch 128/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5794    \n",
      "Epoch 129/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5833    \n",
      "Epoch 130/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5837    \n",
      "Epoch 131/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5866    \n",
      "Epoch 132/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.5782    \n",
      "Epoch 133/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5809    \n",
      "Epoch 134/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.5867    \n",
      "Epoch 135/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5768    \n",
      "Epoch 136/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5680    \n",
      "Epoch 137/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.5663    \n",
      "Epoch 138/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.5610    \n",
      "Epoch 139/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5668    \n",
      "Epoch 140/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5635    \n",
      "Epoch 141/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5679    \n",
      "Epoch 142/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5716    \n",
      "Epoch 143/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5663    \n",
      "Epoch 144/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5570    \n",
      "Epoch 145/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5627    \n",
      "Epoch 146/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5586    \n",
      "Epoch 147/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5547    \n",
      "Epoch 148/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5622    \n",
      "Epoch 149/150\n",
      "4602/4602 [==============================] - 12s - loss: 2.5529    \n",
      "Epoch 150/150\n",
      "4602/4602 [==============================] - 11s - loss: 2.5541    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0b666d668>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.45618123e-10   1.55483781e-09   5.45691103e-10   5.13428240e-05\n",
      "   9.80473123e-06   1.64253322e-06   4.87833418e-09   1.42261060e-05\n",
      "   1.03441855e-09   9.19953163e-06   4.85206771e-11   2.54906426e-11\n",
      "   9.81084168e-01   6.37230215e-08   3.29789805e-06   4.84058000e-05\n",
      "   8.57673222e-05   9.99808013e-01   5.95502936e-09]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "x=model.predict(X_train)\n",
    "print(x[0])\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.21870148e-20   2.26754298e-19   1.21623343e-15   3.28240490e-10\n",
      "    9.98828113e-01   3.05546848e-13   2.84548332e-18   1.82239432e-11\n",
      "    7.62847739e-20   4.80332433e-17   4.60151685e-23   1.32625921e-18\n",
      "    1.12270557e-08   1.53182331e-18   1.32363913e-21   2.27143150e-14\n",
      "    2.20687289e-05   1.00000000e+00   2.75570734e-18]\n",
      " [  7.35758229e-15   7.04969090e-15   1.68052218e-14   1.16124852e-06\n",
      "    9.97464895e-01   1.42212889e-06   4.53724713e-17   4.87392572e-05\n",
      "    1.35292837e-14   7.76985101e-15   7.63019508e-20   4.38535892e-15\n",
      "    4.17410746e-07   1.24451068e-15   1.74597613e-19   2.80547493e-14\n",
      "    6.71490000e-07   1.00000000e+00   6.79645346e-15]\n",
      " [  3.40036521e-26   3.99924707e-27   1.14061381e-27   1.05312686e-11\n",
      "    9.33688023e-12   9.81379569e-01   4.39402072e-28   9.95947897e-01\n",
      "    2.69065941e-26   4.22413884e-29   2.63622886e-29   9.19371237e-25\n",
      "    1.60914522e-17   1.91976228e-25   0.00000000e+00   9.45936197e-31\n",
      "    8.90053002e-07   1.00000000e+00   7.76284552e-26]\n",
      " [  1.90302756e-26   9.80608401e-26   7.33320978e-21   9.93743213e-11\n",
      "    3.43998545e-05   2.07420599e-04   7.49616185e-26   9.96948898e-01\n",
      "    4.96088492e-25   3.12167595e-31   1.20962806e-27   3.82648420e-21\n",
      "    1.21854094e-16   7.94739447e-25   0.00000000e+00   2.92057543e-31\n",
      "    4.46354306e-07   1.00000000e+00   3.97051499e-28]\n",
      " [  6.24795521e-19   6.69643289e-19   8.80319212e-21   9.28675536e-09\n",
      "    5.45936814e-07   9.96653259e-01   2.69989005e-20   1.61254522e-03\n",
      "    1.09998581e-18   5.75686849e-21   5.44550304e-22   3.69380508e-20\n",
      "    1.99627181e-07   4.49709776e-17   5.51423416e-22   3.51930816e-19\n",
      "    1.67682743e-10   1.00000000e+00   1.83309285e-19]]\n"
     ]
    }
   ],
   "source": [
    "Y_predicted = model.predict(X_test)\n",
    "print(Y_predicted[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted_binary = np.array([[1 if Y_ele > 0.5 else 0 for Y_ele in Y_row] for Y_row in Y_predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_predicted_binary[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"predicted.npy\", Y_predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
